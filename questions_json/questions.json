{
    "1": {
        "contents": "\n#### Problem1：数据预处理\n    \n    ##### 题目描述：\n    \n    对一组数据进行预处理，以便后续任务。\n    该数据集包含若干特征，既有数值型特征，也有类别型特征：\n    \n    ##### 任务要求：\n    编写一个 Python 脚本，完成以下步骤：\n    1. def OneHot(Y: np.array) -> np.array: 对类别特征进行one hot编码\n    2. def centred(X: np.array) -> np.array: 将每个特征减去其均值\n    3. def normalize(X: np.array) -> np.array: 将数据转换为均值为0，标准差为1的标准正态分布",
        "pretext": "import numpy as np\n\nX = [[1., 2., 3.],\n     [4., 5., 6.],\n     [7., 8., 9., ],\n     [10., 11., 12., ]]\nY = ['A', 'B', 'C', 'D', 'B', 'C', 'D']\n\n\ndef OneHotEncoding(Y: np.ndarray) -> np.ndarray:\n\n\n\ndef centred(X: np.ndarray) -> np.ndarray:\n\n\n\ndef normalize(X: np.ndarray) -> np.ndarray:\n",
        "output": "[[1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\n[[-4.5 -4.5 -4.5]\n [-1.5 -1.5 -1.5]\n [ 1.5  1.5  1.5]\n [ 4.5  4.5  4.5]]\n[[-1.34164079 -1.34164079 -1.34164079]\n [-0.4472136  -0.4472136  -0.4472136 ]\n [ 0.4472136   0.4472136   0.4472136 ]\n [ 1.34164079  1.34164079  1.34164079]]\n",
        "answer": "import numpy as np\n# 三维数组\nX = np.array([[1., 2., 3.],\n     [4., 5., 6.],\n     [7., 8., 9., ],\n     [10., 11., 12., ]])\n\nY = np.array(['A', 'B', 'C', 'D', 'B', 'C', 'D'])\n\n\ndef OneHotEncoding(Y: np.ndarray) -> np.ndarray:\n    # 获取唯一类别和类别数量\n    unique_categories = np.unique(Y)\n    num_categories = unique_categories.shape[0]\n    # {类别:索引}\n    category_to_index = {category: index for index, category in enumerate(unique_categories)}\n    # 初始化结果矩阵\n    one_hot_Y = np.zeros((Y.shape[0], unique_categories.shape[0]))\n    # 填充1\n    for idx, category in enumerate(Y):\n        one_hot_Y[idx, category_to_index[category]] = 1\n    return one_hot_Y\n\n\ndef centred(X: np.ndarray) -> np.ndarray:\n    mean_X = np.mean(X, axis=0)\n    # 数据-均值\n    features_centered = X - mean_X\n    return features_centered\n\n\ndef normalize(X: np.ndarray) -> np.ndarray:\n    # 计算每个特征的均值\n    mean = np.mean(X, axis=0)\n    # 计算每个特征的标准差\n    std_dev = np.std(X, axis=0)\n    # 对数据进行正态化处理\n    normalized_data = (X - mean) / std_dev\n    return normalized_data\n\n\nprint(OneHotEncoding(Y))\nprint(centred(X))\nprint(normalize(X))"
    },
    "2": {
        "contents": "\n    接下来我们完成激活值a的计算，即A = f(WX+bw) 过程\n    \n    要求：\n    1. 完成三个激活函数定义\n    2. 完成计算激活值函数定义\n      \n    提示：numpy中向量叉乘函数 X1.dot(X2)\n    ",
        "pretext": "import numpy as np\ndef compute_activation(W: np.ndarray, b: np.ndarray, X: np.ndarray,activation_func) -> np.ndarray:\n    \"\"\"\n    Z = AX+b\n    A=f(Z)\"\"\"\n\ndef sigmoid(Z:np.ndarray) -> np.ndarray:\n    pass\n\ndef ReLU(Z:np.ndarray) -> np.ndarray:\n    pass\n\ndef tanh(Z:np.ndarray) -> np.ndarray:\n    pass\n    \nW = np.array([[0.1, 0.2],\n              [0.3, 0.4]])\nb = np.array([0.5, 0.6])\nX = np.array([1, 2])\n\nprint(\"激活值：\", compute_activation(W, b, X, sigmoid))\n    ",
        "answer": "import numpy as np\ndef compute_activation(W: np.ndarray, b: np.ndarray, X: np.ndarray,activation_func) -> np.ndarray:\n    Z = W.dot(X) + b\n    A = activation_func(Z)\n    return A\n\ndef sigmoid(Z:np.ndarray) -> np.ndarray:\n    return 1/(1+np.exp(-Z))\n\ndef ReLU(Z:np.ndarray) -> np.ndarray:\n    \"\"\"返回0(z<0)或z(z>=0)\"\"\"\n    return np.maximum(0,Z)\n\ndef tanh(Z:np.ndarray) -> np.ndarray:\n    \"\"\"-1到1\"\"\"\n    return np.tanh(Z)\n\nW = np.array([[0.1, 0.2],\n              [0.3, 0.4]])\nb = np.array([0.5, 0.6])\nX = np.array([1, 2])\n\nprint(compute_activation(W, b, X, sigmoid))",
        "output": "[0.73105858 0.84553473]\n"
    },
    "3": {
        "contents": "\n    接下来我们构造一个简单的前向传播模型，包含一个隐藏层\n    从输入层到隐藏层用ReLu激活，从隐藏层到输出用sigmoid激活\n    要求：\n    1. 完成三个激活函数定义\n    2. 完成计算激活值函数定义\n      \n    \n    ",
        "pretext": "\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef relu(x):\n    return np.maximum(0, x)\n    \ndef forward_propagation(W1: np.ndarray, b1: np.ndarray, func_1, W2: np.ndarray, b2: np.ndarray, func_2, X: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n\n    return Z1, A1, Z2, A2\n\nX = np.array([1, 2, 3])\n\n# 输入层到隐藏层\nW1 = np.array([[0.1, 0.2, 0.3],\n               [0.4, 0.5, 0.6],\n               [0.7, 0.8, 0.9],\n               [1.0, 1.1, 1.2]])  # 权重矩阵 (4, 3)\nb1 = np.array([0.1, 0.2, 0.3, 0.4])  # 偏置向量 (4,)\n\n# 隐藏层到输出层\nW2 = np.array([[0.1, 0.2, 0.3, 0.4],\n               [0.5, 0.6, 0.7, 0.8]])  # 权重矩阵 (2, 4)\nb2 = np.array([0.1, 0.2])  # 偏置向量 (2,)\n\nZ1, A1, Z2, A2 = forward_propagation(W1, b1, relu, W2, b2, sigmoid, X)\nprint(A2)\n    \n    ",
        "answer": "\n\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef relu(x):\n    return np.maximum(0, x)\ndef forward_propagation(W1: np.ndarray, b1: np.ndarray, func_1, W2: np.ndarray, b2: np.ndarray, func_2, X: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n    Z1 = W1.dot(X) + b1\n    A1 = func_1(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = func_2(Z2)\n    return Z1, A1, Z2, A2\n\n\n# 输入数据\nX = np.array([1, 2, 3])\n\n# 输入层到隐藏层\nW1 = np.array([[0.1, 0.2, 0.3],\n               [0.4, 0.5, 0.6],\n               [0.7, 0.8, 0.9],\n               [1.0, 1.1, 1.2]])  # 权重矩阵 (4, 3)\nb1 = np.array([0.1, 0.2, 0.3, 0.4])  # 偏置向量 (4,)\n\n# 隐藏层到输出层\nW2 = np.array([[0.1, 0.2, 0.3, 0.4],\n               [0.5, 0.6, 0.7, 0.8]])  # 权重矩阵 (2, 4)\nb2 = np.array([0.1, 0.2])  # 偏置向量 (2,)\n\n# 执行前向传播\nZ1, A1, Z2, A2 = forward_propagation(W1, b1, relu, W2, b2, sigmoid, X)\nprint(A2)\n    ",
        "output": "[0.99550373 0.99999612]\n"
    },
    "4": {
        "contents": "\n    #### 现在请你自己尝试定义反向传播函数\n    \n    // 这一题不需要任何输出\n    \n    ",
        "pretext": "\n\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\ndef forward_propagation(W1, b1, func_1, W2, b2, func_2, X):\n    \n    return Z1, A1, Z2, A2\n\ndef backward_propagation(X, Y, Z1, A1, Z2, A2, W1, b1, W2, b2, func1_derive,func2_derive,learning_rate=0.01):\n\n    return W1,b1,W2,b2\n\ndef compute_loss(A2,Y):\n    return np.mean((A2 - Y) ** 2)\n\n\nX = np.array([[1, 2, 3], [4, 5, 6]])  # 输入数据 (2, 3)\nY = np.array([[0, 1, 0]])             # 真实标签 (1, 3)\n\nW1 = np.array([[0.1, 0.2],\n               [0.3, 0.4],\n               [0.5, 0.6],\n               [0.7, 0.8]])  # 权重矩阵 (4, 2)\nb1 = np.array([[0.1], [0.2], [0.3], [0.4]])  # 偏置向量 (4, 1)\n\nW2 = np.array([[0.1, 0.2, 0.3, 0.4]])  # 权重矩阵 (1, 4)\nb2 = np.array([[0.1]])  # 偏置向量 (1, 1)\n\n\nZ1, A1, Z2, A2 = forward_propagation(W1, b1, sigmoid, W2, b2, sigmoid, X)\n\n# 计算初始损失\ninitial_loss = compute_loss(A2, Y)\n\n# 反向传播并更新参数\nW1, b1, W2, b2 = backward_propagation(X, Y, Z1, A1, Z2, A2, W1, b1, W2, b2,sigmoid_derivative,sigmoid_derivative)\nZ1, A1, Z2, A2 = forward_propagation(W1, b1, sigmoid, W2, b2, sigmoid, X)\nupdated_loss = compute_loss(A2, Y)  \n    ",
        "answer": "\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\ndef forward_propagation(W1, b1, func_1, W2, b2, func_2, X):\n    Z1 = W1.dot(X) + b1\n    A1 = func_1(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = func_2(Z2)\n    return Z1, A1, Z2, A2\n\ndef backward_propagation(X, Y, Z1, A1, Z2, A2, W1, b1, W2, b2, func1_derive,func2_derive,learning_rate=0.01):\n    m = X.shape[1]\n    dZ2 = A2-Y #误差\n    dW2 = 1/m * np.dot(dZ2,A1.T)\n    db2 = 1/m * np.sum(dZ2,axis=1,keepdims=True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * func1_derive(Z1)\n    dW1 = 1 / m * np.dot(dZ1, X.T)\n    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n\n    W2 -= learning_rate*dW2\n    b2 -= learning_rate*db2\n    W1 -= learning_rate*dW1\n    b1 -= learning_rate*db1\n\n    return W1,b1,W2,b2\n\ndef compute_loss(A2,Y):\n    return np.mean((A2 - Y) ** 2)\n\n\nX = np.array([[1, 2, 3], [4, 5, 6]])  # 输入数据 (2, 3)\nY = np.array([[0, 1, 0]])             # 真实标签 (1, 3)\n\nW1 = np.array([[0.1, 0.2],\n               [0.3, 0.4],\n               [0.5, 0.6],\n               [0.7, 0.8]])  # 权重矩阵 (4, 2)\nb1 = np.array([[0.1], [0.2], [0.3], [0.4]])  # 偏置向量 (4, 1)\n\nW2 = np.array([[0.1, 0.2, 0.3, 0.4]])  # 权重矩阵 (1, 4)\nb2 = np.array([[0.1]])  # 偏置向量 (1, 1)\n\n\nZ1, A1, Z2, A2 = forward_propagation(W1, b1, sigmoid, W2, b2, sigmoid, X)\n\n# 计算初始损失\ninitial_loss = compute_loss(A2, Y)\n\n# 反向传播并更新参数\nW1, b1, W2, b2 = backward_propagation(X, Y, Z1, A1, Z2, A2, W1, b1, W2, b2,sigmoid_derivative,sigmoid_derivative)\n\n# 前向传播\nZ1, A1, Z2, A2 = forward_propagation(W1, b1, sigmoid, W2, b2, sigmoid, X)\n\n# 计算更新后的损失\nupdated_loss = compute_loss(A2, Y)\n    \n    ",
        "output": ""
    }
}